{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "eebed8d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:\n",
      "['', '[UNK]', 'i', 'love', 'learning', 'programming', 'enjoy', 'deep', 'machine']\n",
      "\n",
      "Vectorized Data (Sequences):\n",
      "tf.Tensor(\n",
      "[[2 3 8 4]\n",
      " [2 3 7 4]\n",
      " [2 6 7 4]\n",
      " [2 6 5 0]\n",
      " [2 3 5 0]], shape=(5, 4), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "import keras\n",
    "\n",
    "# Sample sentences\n",
    "sentences = [\n",
    "    'i love machine learning',\n",
    "    'i love deep learning',\n",
    "    'i enjoy deep learning',\n",
    "    'i enjoy programming',\n",
    "    'i love programming'\n",
    "]\n",
    "\n",
    "# Create a TextVectorization layer\n",
    "# max_tokens: The maximum size of the vocabulary.\n",
    "# output_sequence_length: The length of the output sequences.\n",
    "vectorize_layer = keras.layers.TextVectorization(\n",
    "    max_tokens=10, \n",
    "    output_sequence_length=4\n",
    ")\n",
    "\n",
    "# Adapt the layer to your data\n",
    "# This is where the vocabulary is built\n",
    "vectorize_layer.adapt(sentences)\n",
    "\n",
    "# Convert sentences to integer sequences\n",
    "vectorized_data = vectorize_layer(tf.constant(sentences))\n",
    "\n",
    "print(\"Vocabulary:\")\n",
    "print(vectorize_layer.get_vocabulary())\n",
    "print(\"\\nVectorized Data (Sequences):\")\n",
    "print(vectorized_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e82c4bcc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (<span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)                 │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)           │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (\u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m4\u001b[0m)                 │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)           │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from keras.layers import Embedding\n",
    "    \n",
    "# Get the vocabulary size and embedding dimension\n",
    "vocab_size = vectorize_layer.vocabulary_size()\n",
    "embedding_dim = 64  # This is a hyperparameter you can tune\n",
    "\n",
    "# Create a simple model\n",
    "model = keras.Sequential([\n",
    "    vectorize_layer,  # The TextVectorization layer we created earlier\n",
    "    Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim,\n",
    "    )\n",
    "])\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "009c0b23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of word embeddings: (5, 4, 64)\n",
      "Embedding vectors:\n",
      " tf.Tensor(\n",
      "[[[-0.02249515 -0.04291451  0.00474689 ... -0.02096723 -0.04835026\n",
      "   -0.0268787 ]\n",
      "  [-0.04161547 -0.01314483  0.01210267 ... -0.01529636 -0.0094137\n",
      "   -0.02867313]\n",
      "  [-0.04098078 -0.0174413   0.02731076 ... -0.01953267 -0.01705965\n",
      "    0.01653096]\n",
      "  [-0.02212741 -0.00045705  0.00820044 ...  0.04049009 -0.02096406\n",
      "   -0.03435572]]\n",
      "\n",
      " [[-0.02249515 -0.04291451  0.00474689 ... -0.02096723 -0.04835026\n",
      "   -0.0268787 ]\n",
      "  [-0.04161547 -0.01314483  0.01210267 ... -0.01529636 -0.0094137\n",
      "   -0.02867313]\n",
      "  [ 0.00825231  0.03822804  0.02113021 ...  0.02787054 -0.03367592\n",
      "    0.03322575]\n",
      "  [-0.02212741 -0.00045705  0.00820044 ...  0.04049009 -0.02096406\n",
      "   -0.03435572]]\n",
      "\n",
      " [[-0.02249515 -0.04291451  0.00474689 ... -0.02096723 -0.04835026\n",
      "   -0.0268787 ]\n",
      "  [ 0.04473734  0.0164606  -0.02382165 ... -0.00968386  0.01655009\n",
      "   -0.0035928 ]\n",
      "  [ 0.00825231  0.03822804  0.02113021 ...  0.02787054 -0.03367592\n",
      "    0.03322575]\n",
      "  [-0.02212741 -0.00045705  0.00820044 ...  0.04049009 -0.02096406\n",
      "   -0.03435572]]\n",
      "\n",
      " [[-0.02249515 -0.04291451  0.00474689 ... -0.02096723 -0.04835026\n",
      "   -0.0268787 ]\n",
      "  [ 0.04473734  0.0164606  -0.02382165 ... -0.00968386  0.01655009\n",
      "   -0.0035928 ]\n",
      "  [ 0.03452691  0.04137261  0.03045902 ...  0.02528055 -0.01308429\n",
      "    0.0303377 ]\n",
      "  [ 0.00238354  0.04857427 -0.02413175 ...  0.01317925  0.03724605\n",
      "   -0.03527945]]\n",
      "\n",
      " [[-0.02249515 -0.04291451  0.00474689 ... -0.02096723 -0.04835026\n",
      "   -0.0268787 ]\n",
      "  [-0.04161547 -0.01314483  0.01210267 ... -0.01529636 -0.0094137\n",
      "   -0.02867313]\n",
      "  [ 0.03452691  0.04137261  0.03045902 ...  0.02528055 -0.01308429\n",
      "    0.0303377 ]\n",
      "  [ 0.00238354  0.04857427 -0.02413175 ...  0.01317925  0.03724605\n",
      "   -0.03527945]]], shape=(5, 4, 64), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "# Create a dummy input to get the output of the embedding layer\n",
    "sample_input = tf.constant(sentences)\n",
    "# Pass the input through the model to get the embedding vectors\n",
    "word_embeddings = model(sample_input)\n",
    "\n",
    "# Print the shape and first few embedding vectors\n",
    "print(\"Shape of word embeddings:\", word_embeddings.shape)\n",
    "print(\"Embedding vectors:\\n\", word_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ed25fe3c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TextVectorization</span>)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)         │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ ?                      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling1D</span>)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ ?                      │   <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ text_vectorization_1            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m4\u001b[0m)              │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTextVectorization\u001b[0m)             │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)         │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ global_average_pooling1d_1      │ ?                      │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mGlobalAveragePooling1D\u001b[0m)        │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ ?                      │   \u001b[38;5;34m0\u001b[0m (unbuilt) │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step - accuracy: 0.2500 - loss: 0.6955 - val_accuracy: 1.0000 - val_loss: 0.6832 - learning_rate: 0.0010\n",
      "Epoch 2/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step - accuracy: 0.2500 - loss: 0.6929 - val_accuracy: 1.0000 - val_loss: 0.6834 - learning_rate: 0.0010\n",
      "Epoch 3/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 56ms/step - accuracy: 0.5000 - loss: 0.6904 - val_accuracy: 1.0000 - val_loss: 0.6835 - learning_rate: 0.0010\n",
      "Epoch 4/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 58ms/step - accuracy: 0.7500 - loss: 0.6879 - val_accuracy: 1.0000 - val_loss: 0.6837 - learning_rate: 0.0010\n",
      "Epoch 5/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6854 - val_accuracy: 1.0000 - val_loss: 0.6839 - learning_rate: 0.0010\n",
      "Epoch 6/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.6829 - val_accuracy: 1.0000 - val_loss: 0.6842 - learning_rate: 0.0010\n",
      "Epoch 7/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step - accuracy: 1.0000 - loss: 0.6804 - val_accuracy: 1.0000 - val_loss: 0.6842 - learning_rate: 2.0000e-04\n",
      "Epoch 8/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step - accuracy: 1.0000 - loss: 0.6799 - val_accuracy: 1.0000 - val_loss: 0.6843 - learning_rate: 2.0000e-04\n",
      "Epoch 9/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6794 - val_accuracy: 1.0000 - val_loss: 0.6844 - learning_rate: 2.0000e-04\n",
      "Epoch 10/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step - accuracy: 1.0000 - loss: 0.6789 - val_accuracy: 1.0000 - val_loss: 0.6844 - learning_rate: 2.0000e-04\n",
      "Epoch 11/100\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step - accuracy: 1.0000 - loss: 0.6784 - val_accuracy: 1.0000 - val_loss: 0.6845 - learning_rate: 2.0000e-04\n"
     ]
    }
   ],
   "source": [
    "# We need labels for classification\n",
    "labels = tf.constant([0, 0, 1, 1, 0]) # 0 for \"learning\", 1 for \"programming\"\n",
    "\n",
    "# A simple classification model\n",
    "classification_model = keras.Sequential([\n",
    "    vectorize_layer,\n",
    "    keras.layers.Embedding(\n",
    "        input_dim=vocab_size,\n",
    "        output_dim=embedding_dim\n",
    "    ),\n",
    "    keras.layers.GlobalAveragePooling1D(), # A layer to reduce the dimensions\n",
    "    keras.layers.Dense(1, activation='sigmoid') # A dense layer for binary classification\n",
    "])\n",
    "\n",
    "classification_model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='binary_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "classification_model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(patience=10, restore_best_weights=True, monitor=\"val_loss\"),\n",
    "    keras.callbacks.ReduceLROnPlateau(patience=5, factor=0.2, monitor=\"val_loss\")\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = classification_model.fit(\n",
    "    tf.constant(sentences),\n",
    "    labels,\n",
    "    epochs=100,\n",
    "    callbacks=callbacks,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
